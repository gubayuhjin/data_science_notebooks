{"nbformat_minor": 0, "cells": [{"source": "## Feature Selection with scikit-learn (sklearn)", "cell_type": "markdown", "metadata": {"raw_mimetype": "text/latex"}}, {"source": "Feature extraction is one of the essential setp in Data Science/Machine Learning and Data Mining excercises. Effective use of feature extraction techniques helps a Data Scientist to build the bset model. This note is intent to give a brief over view on feature selection with scikit-learn (sklearn). The result of a feature selection excercise is to find the most important and descriptive feature from a given data.", "cell_type": "markdown", "metadata": {}}, {"source": "#### Find K-Best features for classification and regression\nThe first methos which we are going to expore is the selecting the K-best featres using the SelectKBest utility in sklearn. We will use the famous IRIS two class data-set. \n\nThe first example we are going to look is feature selection for classification.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 52, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\ndef select_kbest_clf(data_frame, target, k=2):\n    \"\"\"\n    Selecting K-Best features for classification\n    :param data_frame: A pandas dataFrame with the training data\n    :param target: target variable name in DataFrame\n    :param k: desired number of features from the data\n    :returns feature_scores: scores for each feature in the data as \n    pandas DataFrame\n    \"\"\"\n    feat_selector = SelectKBest(f_classif, k=k)\n    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n    \n    return feat_scores\n\niris_data = pd.read_csv(\"/resources/iris.csv\")\n\nkbest_feat = select_kbest_clf(iris_data, \"Class\", k=2)\nkbest_feat = kbest_feat.sort([\"F Score\", \"P Value\"], ascending=[False, False])\nkbest_feat\n", "outputs": [{"execution_count": 52, "output_type": "execute_result", "data": {"text/plain": "       F Score       P Value Support     Attribute\n2  2498.618817  1.504801e-71    True  petal-length\n3  1830.624469  3.230375e-65    True   petal-width\n0   236.735022  6.892546e-28   False  sepal-length\n1    41.607003  4.246355e-09   False   sepal-width\n\n[4 rows x 4 columns]", "text/html": "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F Score</th>\n      <th>P Value</th>\n      <th>Support</th>\n      <th>Attribute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td> 2498.618817</td>\n      <td> 1.504801e-71</td>\n      <td>  True</td>\n      <td> petal-length</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td> 1830.624469</td>\n      <td> 3.230375e-65</td>\n      <td>  True</td>\n      <td>  petal-width</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>  236.735022</td>\n      <td> 6.892546e-28</td>\n      <td> False</td>\n      <td> sepal-length</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>   41.607003</td>\n      <td> 4.246355e-09</td>\n      <td> False</td>\n      <td>  sepal-width</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows \u00d7 4 columns</p>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "##### What just happened ?\n\nThe select_kbest function accepts a pandas DataFrame, and target variable name and k as parameters. First we create a SelectKBest object with estimator as f_classif (because we are working with a classification problem). The we are fitting the model with the data. Once we fit the model information on feature importnace will be available in the fitted model. The Annova F score of the features are accesible thorugh the scores_ attributes and the p-values are avaiale thorugh the pvalues_. The get_support function will return a bool value if a feature is selected. \n\nNow the question is how can I determine which feature is selected? The easy way is that if the Support is Tru those features are are good. The higher the F Score and the lesser the p-values the feature is best. \n\nLet's examine the results we obtained from the iris data. The attributes 'petal-length' and 'petal-width' got higher F Score and lesser P Value; and Support is true. So those feature are important comapred to other features. To understand the real-power of this methos you have to check this with a data with more diamensions.\n\n##### Next ....\nIn the next example we can try to see how we can apply this technique to a regression problem. Basically there is not much difference in the code. We will change the estimator to f_regression. We can try this with the Boston house price dataset.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\ndef select_kbest_reg(data_frame, target, k=5):\n    \"\"\"\n    Selecting K-Best features for regression\n    :param data_frame: A pandas dataFrame with the training data\n    :param target: target variable name in DataFrame\n    :param k: desired number of features from the data\n    :returns feature_scores: scores for each feature in the data as \n    pandas DataFrame\n    \"\"\"\n    feat_selector = SelectKBest(f_regression, k=k)\n    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n    \n    return feat_scores\n\nboston = pd.read_csv(\"/resources/boston.csv\")\n\nkbest_feat = select_kbest_reg(boston, \"price\", k=5)\n\nkbest_feat = kbest_feat.sort([\"F Score\", \"P Value\"], ascending=[False, False])\nkbest_feat\n", "outputs": [{"execution_count": 53, "output_type": "execute_result", "data": {"text/plain": "       F Score       P Value Support Attribute\n12  601.617871  5.081103e-88    True        12\n5   471.846740  2.487229e-74    True         5\n10  175.105543  1.609509e-34    True        10\n2   153.954883  4.900260e-31    True         2\n9   141.761357  5.637734e-29    True         9\n4   112.591480  7.065042e-24   False         4\n0    88.151242  2.083550e-19   False         0\n8    85.914278  5.465933e-19   False         8\n6    83.477459  1.569982e-18   False         6\n1    75.257642  5.713584e-17   False         1\n11   63.054229  1.318113e-14   False        11\n7    33.579570  1.206612e-08   False         7\n3    15.971512  7.390623e-05   False         3\n\n[13 rows x 4 columns]", "text/html": "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F Score</th>\n      <th>P Value</th>\n      <th>Support</th>\n      <th>Attribute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td> 601.617871</td>\n      <td> 5.081103e-88</td>\n      <td>  True</td>\n      <td> 12</td>\n    </tr>\n    <tr>\n      <th>5 </th>\n      <td> 471.846740</td>\n      <td> 2.487229e-74</td>\n      <td>  True</td>\n      <td>  5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td> 175.105543</td>\n      <td> 1.609509e-34</td>\n      <td>  True</td>\n      <td> 10</td>\n    </tr>\n    <tr>\n      <th>2 </th>\n      <td> 153.954883</td>\n      <td> 4.900260e-31</td>\n      <td>  True</td>\n      <td>  2</td>\n    </tr>\n    <tr>\n      <th>9 </th>\n      <td> 141.761357</td>\n      <td> 5.637734e-29</td>\n      <td>  True</td>\n      <td>  9</td>\n    </tr>\n    <tr>\n      <th>4 </th>\n      <td> 112.591480</td>\n      <td> 7.065042e-24</td>\n      <td> False</td>\n      <td>  4</td>\n    </tr>\n    <tr>\n      <th>0 </th>\n      <td>  88.151242</td>\n      <td> 2.083550e-19</td>\n      <td> False</td>\n      <td>  0</td>\n    </tr>\n    <tr>\n      <th>8 </th>\n      <td>  85.914278</td>\n      <td> 5.465933e-19</td>\n      <td> False</td>\n      <td>  8</td>\n    </tr>\n    <tr>\n      <th>6 </th>\n      <td>  83.477459</td>\n      <td> 1.569982e-18</td>\n      <td> False</td>\n      <td>  6</td>\n    </tr>\n    <tr>\n      <th>1 </th>\n      <td>  75.257642</td>\n      <td> 5.713584e-17</td>\n      <td> False</td>\n      <td>  1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>  63.054229</td>\n      <td> 1.318113e-14</td>\n      <td> False</td>\n      <td> 11</td>\n    </tr>\n    <tr>\n      <th>7 </th>\n      <td>  33.579570</td>\n      <td> 1.206612e-08</td>\n      <td> False</td>\n      <td>  7</td>\n    </tr>\n    <tr>\n      <th>3 </th>\n      <td>  15.971512</td>\n      <td> 7.390623e-05</td>\n      <td> False</td>\n      <td>  3</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows \u00d7 4 columns</p>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##### Select features according to a percentile of the highest scores.\n\nThe next trick we are going to explore is 'SelectPercentile' based feature selection. This technique will return the features base on percentile of the highest score. Let's see it in action with Boston data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 54, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn.feature_selection import SelectPercentile, f_regression\n\n\ndef select_percentile(data_frame, target, percentile=15):\n    \"\"\"\n    Percentile based feature selection for regression\n    :param data_frame: A pandas dataFrame with the training data\n    :param target: target variable name in DataFrame\n    :param k: desired number of features from the data\n    :returns feature_scores: scores for each feature in the data as \n    pandas DataFrame\n    \"\"\"\n    feat_selector = SelectPercentile(f_regression, percentile=percentile)\n    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n    \n    return feat_scores\n\nboston = pd.read_csv(\"/resources/boston.csv\")\n\nkbest_feat = select_percentile(boston, \"price\", percentile=50)\n\nkbest_feat = kbest_feat.sort([\"F Score\", \"P Value\"], ascending=[False, False])\nkbest_feat", "outputs": [{"execution_count": 54, "output_type": "execute_result", "data": {"text/plain": "       F Score       P Value Support Attribute\n12  601.617871  5.081103e-88    True        12\n5   471.846740  2.487229e-74    True         5\n10  175.105543  1.609509e-34    True        10\n2   153.954883  4.900260e-31    True         2\n9   141.761357  5.637734e-29    True         9\n4   112.591480  7.065042e-24    True         4\n0    88.151242  2.083550e-19   False         0\n8    85.914278  5.465933e-19   False         8\n6    83.477459  1.569982e-18   False         6\n1    75.257642  5.713584e-17   False         1\n11   63.054229  1.318113e-14   False        11\n7    33.579570  1.206612e-08   False         7\n3    15.971512  7.390623e-05   False         3\n\n[13 rows x 4 columns]", "text/html": "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F Score</th>\n      <th>P Value</th>\n      <th>Support</th>\n      <th>Attribute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td> 601.617871</td>\n      <td> 5.081103e-88</td>\n      <td>  True</td>\n      <td> 12</td>\n    </tr>\n    <tr>\n      <th>5 </th>\n      <td> 471.846740</td>\n      <td> 2.487229e-74</td>\n      <td>  True</td>\n      <td>  5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td> 175.105543</td>\n      <td> 1.609509e-34</td>\n      <td>  True</td>\n      <td> 10</td>\n    </tr>\n    <tr>\n      <th>2 </th>\n      <td> 153.954883</td>\n      <td> 4.900260e-31</td>\n      <td>  True</td>\n      <td>  2</td>\n    </tr>\n    <tr>\n      <th>9 </th>\n      <td> 141.761357</td>\n      <td> 5.637734e-29</td>\n      <td>  True</td>\n      <td>  9</td>\n    </tr>\n    <tr>\n      <th>4 </th>\n      <td> 112.591480</td>\n      <td> 7.065042e-24</td>\n      <td>  True</td>\n      <td>  4</td>\n    </tr>\n    <tr>\n      <th>0 </th>\n      <td>  88.151242</td>\n      <td> 2.083550e-19</td>\n      <td> False</td>\n      <td>  0</td>\n    </tr>\n    <tr>\n      <th>8 </th>\n      <td>  85.914278</td>\n      <td> 5.465933e-19</td>\n      <td> False</td>\n      <td>  8</td>\n    </tr>\n    <tr>\n      <th>6 </th>\n      <td>  83.477459</td>\n      <td> 1.569982e-18</td>\n      <td> False</td>\n      <td>  6</td>\n    </tr>\n    <tr>\n      <th>1 </th>\n      <td>  75.257642</td>\n      <td> 5.713584e-17</td>\n      <td> False</td>\n      <td>  1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>  63.054229</td>\n      <td> 1.318113e-14</td>\n      <td> False</td>\n      <td> 11</td>\n    </tr>\n    <tr>\n      <th>7 </th>\n      <td>  33.579570</td>\n      <td> 1.206612e-08</td>\n      <td> False</td>\n      <td>  7</td>\n    </tr>\n    <tr>\n      <th>3 </th>\n      <td>  15.971512</td>\n      <td> 7.390623e-05</td>\n      <td> False</td>\n      <td>  3</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows \u00d7 4 columns</p>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##### Univarite feature selection \nThe next method we are going to expore is univarite feature selection. We will use the same Boston data for this example also.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 55, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn.feature_selection import GenericUnivariateSelect, f_regression\n\n\ndef select_univarite(data_frame, target, mode='fdr'):\n    \"\"\"\n    Percentile based feature selection for regression\n    :param data_frame: A pandas dataFrame with the training data\n    :param target: target variable name in DataFrame\n    :param k: desired number of features from the data\n    :returns feature_scores: scores for each feature in the data as \n    pandas DataFrame\n    \"\"\"\n    feat_selector = GenericUnivariateSelect(f_regression, mode=mode)\n    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n    \n    return feat_scores\n\nboston = pd.read_csv(\"/resources/boston.csv\")\n\nkbest_feat = select_univarite(boston, \"price\", mode='fpr')\n\nkbest_feat = kbest_feat.sort([\"F Score\", \"P Value\"], ascending=[False, False])\nkbest_feat", "outputs": [{"execution_count": 55, "output_type": "execute_result", "data": {"text/plain": "       F Score       P Value Support Attribute\n12  601.617871  5.081103e-88    True        12\n5   471.846740  2.487229e-74    True         5\n10  175.105543  1.609509e-34    True        10\n2   153.954883  4.900260e-31    True         2\n9   141.761357  5.637734e-29    True         9\n4   112.591480  7.065042e-24    True         4\n0    88.151242  2.083550e-19    True         0\n8    85.914278  5.465933e-19    True         8\n6    83.477459  1.569982e-18    True         6\n1    75.257642  5.713584e-17    True         1\n11   63.054229  1.318113e-14    True        11\n7    33.579570  1.206612e-08    True         7\n3    15.971512  7.390623e-05   False         3\n\n[13 rows x 4 columns]", "text/html": "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F Score</th>\n      <th>P Value</th>\n      <th>Support</th>\n      <th>Attribute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td> 601.617871</td>\n      <td> 5.081103e-88</td>\n      <td>  True</td>\n      <td> 12</td>\n    </tr>\n    <tr>\n      <th>5 </th>\n      <td> 471.846740</td>\n      <td> 2.487229e-74</td>\n      <td>  True</td>\n      <td>  5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td> 175.105543</td>\n      <td> 1.609509e-34</td>\n      <td>  True</td>\n      <td> 10</td>\n    </tr>\n    <tr>\n      <th>2 </th>\n      <td> 153.954883</td>\n      <td> 4.900260e-31</td>\n      <td>  True</td>\n      <td>  2</td>\n    </tr>\n    <tr>\n      <th>9 </th>\n      <td> 141.761357</td>\n      <td> 5.637734e-29</td>\n      <td>  True</td>\n      <td>  9</td>\n    </tr>\n    <tr>\n      <th>4 </th>\n      <td> 112.591480</td>\n      <td> 7.065042e-24</td>\n      <td>  True</td>\n      <td>  4</td>\n    </tr>\n    <tr>\n      <th>0 </th>\n      <td>  88.151242</td>\n      <td> 2.083550e-19</td>\n      <td>  True</td>\n      <td>  0</td>\n    </tr>\n    <tr>\n      <th>8 </th>\n      <td>  85.914278</td>\n      <td> 5.465933e-19</td>\n      <td>  True</td>\n      <td>  8</td>\n    </tr>\n    <tr>\n      <th>6 </th>\n      <td>  83.477459</td>\n      <td> 1.569982e-18</td>\n      <td>  True</td>\n      <td>  6</td>\n    </tr>\n    <tr>\n      <th>1 </th>\n      <td>  75.257642</td>\n      <td> 5.713584e-17</td>\n      <td>  True</td>\n      <td>  1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>  63.054229</td>\n      <td> 1.318113e-14</td>\n      <td>  True</td>\n      <td> 11</td>\n    </tr>\n    <tr>\n      <th>7 </th>\n      <td>  33.579570</td>\n      <td> 1.206612e-08</td>\n      <td>  True</td>\n      <td>  7</td>\n    </tr>\n    <tr>\n      <th>3 </th>\n      <td>  15.971512</td>\n      <td> 7.390623e-05</td>\n      <td> False</td>\n      <td>  3</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows \u00d7 4 columns</p>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "In the example if we change the mode to 'fdr' the algo will find the score based on false discovery rate, 'fpr' false positive rate, 'fwr' family based error, 'percentile' and 'kbest' will do Percentile and KBest based scoring.\n\n##### Family-wise error rate \nThe next method we are going to expore is Family-wise error rate. We will use the same Boston data for this example also.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 56, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn.feature_selection import SelectFwe, f_regression\n\n\ndef select_univarite(data_frame, target):\n    \"\"\"\n    Percentile based feature selection for regression\n    :param data_frame: A pandas dataFrame with the training data\n    :param target: target variable name in DataFrame\n    :param k: desired number of features from the data\n    :returns feature_scores: scores for each feature in the data as \n    pandas DataFrame\n    \"\"\"\n    feat_selector = SelectFwe(f_regression)\n    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n    \n    return feat_scores\n\nboston = pd.read_csv(\"/resources/boston.csv\")\n\nkbest_feat = select_univarite(boston, \"price\")\n\nkbest_feat = kbest_feat.sort([\"F Score\", \"P Value\"], ascending=[False, False])\nkbest_feat", "outputs": [{"execution_count": 56, "output_type": "execute_result", "data": {"text/plain": "       F Score       P Value Support Attribute\n12  601.617871  5.081103e-88    True        12\n5   471.846740  2.487229e-74    True         5\n10  175.105543  1.609509e-34    True        10\n2   153.954883  4.900260e-31    True         2\n9   141.761357  5.637734e-29    True         9\n4   112.591480  7.065042e-24    True         4\n0    88.151242  2.083550e-19    True         0\n8    85.914278  5.465933e-19    True         8\n6    83.477459  1.569982e-18    True         6\n1    75.257642  5.713584e-17    True         1\n11   63.054229  1.318113e-14    True        11\n7    33.579570  1.206612e-08    True         7\n3    15.971512  7.390623e-05    True         3\n\n[13 rows x 4 columns]", "text/html": "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F Score</th>\n      <th>P Value</th>\n      <th>Support</th>\n      <th>Attribute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td> 601.617871</td>\n      <td> 5.081103e-88</td>\n      <td> True</td>\n      <td> 12</td>\n    </tr>\n    <tr>\n      <th>5 </th>\n      <td> 471.846740</td>\n      <td> 2.487229e-74</td>\n      <td> True</td>\n      <td>  5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td> 175.105543</td>\n      <td> 1.609509e-34</td>\n      <td> True</td>\n      <td> 10</td>\n    </tr>\n    <tr>\n      <th>2 </th>\n      <td> 153.954883</td>\n      <td> 4.900260e-31</td>\n      <td> True</td>\n      <td>  2</td>\n    </tr>\n    <tr>\n      <th>9 </th>\n      <td> 141.761357</td>\n      <td> 5.637734e-29</td>\n      <td> True</td>\n      <td>  9</td>\n    </tr>\n    <tr>\n      <th>4 </th>\n      <td> 112.591480</td>\n      <td> 7.065042e-24</td>\n      <td> True</td>\n      <td>  4</td>\n    </tr>\n    <tr>\n      <th>0 </th>\n      <td>  88.151242</td>\n      <td> 2.083550e-19</td>\n      <td> True</td>\n      <td>  0</td>\n    </tr>\n    <tr>\n      <th>8 </th>\n      <td>  85.914278</td>\n      <td> 5.465933e-19</td>\n      <td> True</td>\n      <td>  8</td>\n    </tr>\n    <tr>\n      <th>6 </th>\n      <td>  83.477459</td>\n      <td> 1.569982e-18</td>\n      <td> True</td>\n      <td>  6</td>\n    </tr>\n    <tr>\n      <th>1 </th>\n      <td>  75.257642</td>\n      <td> 5.713584e-17</td>\n      <td> True</td>\n      <td>  1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>  63.054229</td>\n      <td> 1.318113e-14</td>\n      <td> True</td>\n      <td> 11</td>\n    </tr>\n    <tr>\n      <th>7 </th>\n      <td>  33.579570</td>\n      <td> 1.206612e-08</td>\n      <td> True</td>\n      <td>  7</td>\n    </tr>\n    <tr>\n      <th>3 </th>\n      <td>  15.971512</td>\n      <td> 7.390623e-05</td>\n      <td> True</td>\n      <td>  3</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows \u00d7 4 columns</p>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.6", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "celltoolbar": "Raw Cell Format"}}